{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d969fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ab9291-cf12-4f9b-8caa-66a430108ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce380309",
   "metadata": {},
   "source": [
    "# Image Processing with Neural Network\n",
    "## Session 35\n",
    "\n",
    "## Frozen Lake DQN using Pytorch\n",
    "<img src='../../../images/prasami_color_tutorials_small.png' style = 'width:400px;' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe423885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch as T\n",
    "# from utils.frozenlake_pt.helper import to_onehot, moving_average\n",
    "# from utils.frozenlake_pt.deep_q_network import DeepQNetwork\n",
    "# from utils.frozenlake_pt.replay_memory import ReplayBuffer\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register, spec\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9bfc650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\duasp\\\\Neural Network'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f11937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(size,value):\n",
    "    \"\"\"1 hot encoding for observed state\"\"\"\n",
    "    return np.eye(size)[value]\n",
    "\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=np.float32)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e2d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        #self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "        #                             dtype=np.float32)\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        #self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "        #                                 dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4b10b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, name, input_dims, model_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.model_dir = model_dir\n",
    "        self.model_file = os.path.join(self.model_dir, f'{name}.pt')\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256,n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        actions = self.fc5(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        #print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.model_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32bd7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "inpDir = os.path.join( '..', 'input')\n",
    "outDir = '../output'\n",
    "modelDir = 'Neural Network'\n",
    "\n",
    "RANDOM_STATE = 24\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "EPOCHS = 50   # number of cycles to run\n",
    "ALPHA = 0.001\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE = 0.8\n",
    "IMG_HEIGHT = 188\n",
    "IMG_WIDTH = 188\n",
    "\n",
    "\n",
    "# Transformations\n",
    "# flipping\n",
    "FLIP_MODE = \"horizontal_and_vertical\"\n",
    "# Rotation\n",
    "ROTATION_FACTOR = (-0.1, 0.1)\n",
    "# filling mode\n",
    "FILL_MODE = 'nearest'\n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "ES_PATIENCE = 20 # if performance does not improve stop\n",
    "# Learning rates\n",
    "LR_PATIENCE = 10 # if performace is not improving reduce alpha\n",
    "LR_FACTOR = 0.5 # rate of reduction of alpha\n",
    "PATIENCE = 5\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (9, 6),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "EPISODES = 5000\n",
    "\n",
    "# ALPHA = 0.01 #0.85\n",
    "# GAMMA = 0.90\n",
    "# EPSILON = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0e4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA version: 11.7\n",
      "Number of Devices: 1\n",
      "Name of Device: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "def fn_assign_device():\n",
    "    '''\n",
    "        Check for Cuda and Assign device\n",
    "    '''\n",
    "    if T.cuda.is_available():\n",
    "        print ('Using CUDA version:', T.version.cuda)\n",
    "        print ('Number of Devices:', T.cuda.device_count())\n",
    "        print ('Name of Device:', T.cuda.get_device_name(0))\n",
    "        device = T.device('cuda:0')\n",
    "    else:\n",
    "        device = T.device( 'cpu')\n",
    "        print('ERROR: No gpu found')\n",
    "        \n",
    "device = fn_assign_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71bdc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate environment\n",
    "env = gym.make(ENV_NAME, map_name = '8x8', is_slippery = True, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57848fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = env.observation_space.n  # size of state space\n",
    "num_actions = env.action_space.n  # 0 = left; 1 = down; 2 = right;  3 = up\n",
    "\n",
    "lr=0.0001              #'Learning rate for optimizer'\n",
    "eps_min=0.1            #'Minimum value for epsilon in epsilon-greedy action selection')\n",
    "gamma=0.99             #'Discount factor for update equation.'\n",
    "eps_dec=1e-4           #'Linear factor for decreasing epsilon'\n",
    "pretrained = True      # Are we using pretrained model\n",
    "retrain = True         # Are we training further\n",
    "if pretrained:         #'Starting value for epsilon in epsilon-greedy action selection')\n",
    "    eps = 0.5\n",
    "else:\n",
    "    eps=1.0                \n",
    "\n",
    "max_mem=2000           #'Maximum size for memory replay buffer')\n",
    "batch_size=BATCH_SIZE  #'Batch size for replay memory sampling')\n",
    "replace=1000           #'interval for replacing target network'\n",
    "algo='DQNAgent'        #'DQNAgent/DDQNAgent/DuelingDQNAgent/DuelingDDQNAgent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797d199",
   "metadata": {},
   "source": [
    "### DQN Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96219410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, env, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, algo=None, model_dir=modelDir):\n",
    "        '''\n",
    "        Args:\n",
    "            env: Gymnasium environment\n",
    "            gamma: Discount factor\n",
    "            epsilon: Factor for exploration\n",
    "            lr: Learning rate\n",
    "            n_actions: How many actions\n",
    "            input_dims: Size of the environment\n",
    "            mem_size: How many experiences to keep in memory\n",
    "            batch_size: Batch size\n",
    "            eps_min: minimum value of epsilon, Default=0.01 \n",
    "            eps_dec:reduction factor for epsilon, Default=5e-7,\n",
    "            replace: Frequency to replace =1000, \n",
    "            algo=None\n",
    "            model_dir=modelDir\n",
    "        \n",
    "        '''\n",
    "        # Instantiate the class\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.replace_target_cnt = replace\n",
    "        self.algo = algo\n",
    "        self.model_dir = model_dir\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        self.loss = []\n",
    "\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "\n",
    "        # Evaluation network\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims = self.input_dims,\n",
    "                                    name = self.env.spec.id+'_'+self.algo+'_q_eval',\n",
    "                                    model_dir = self.model_dir)\n",
    "            \n",
    "\n",
    "        # target network\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims,\n",
    "                                    name=self.env.spec.id+'_'+self.algo+'_q_next',\n",
    "                                    model_dir=self.model_dir)\n",
    "        \n",
    "        # Best weights\n",
    "        self.q_best = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims,\n",
    "                                    name=self.env.spec.id+'_'+self.algo+'_q_best',\n",
    "                                    model_dir=self.model_dir)\n",
    "        \n",
    "        # Copy weights from Evaluation network\n",
    "        self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "        \n",
    "        if pretrained:\n",
    "            self.load_models()\n",
    "        \n",
    "        \n",
    "\n",
    "    def choose_action(self, observation, evaluation = False):\n",
    "        '''\n",
    "        Args:\n",
    "            observation: State as one hot encoded\n",
    "            evaluation: bool: true when we are evaluating\n",
    "        '''\n",
    "        # are we evaluating\n",
    "        if evaluation:\n",
    "            \n",
    "            self.q_best.eval()   # set in evaluation mode \n",
    "            state = T.tensor(observation, dtype=T.float).to(self.q_eval.device) #Convert to tensor\n",
    "            actions = self.q_best.forward(state) # take a step forward on best values\n",
    "            action = T.argmax(actions).item() # get action\n",
    "        \n",
    "        else:\n",
    "            # for training; epsilon times take random action\n",
    "            if np.random.random() > self.epsilon: # if it is more than epsilon\n",
    "                state = T.tensor(observation, dtype=T.float ).to(self.q_eval.device) #Convert to tensor\n",
    "                actions = self.q_eval.forward(state) # take a step forward on Eval network\n",
    "                action = T.argmax(actions).item() # get action\n",
    "            \n",
    "            else:\n",
    "                action = np.random.choice(self.action_space) # random action\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        '''\n",
    "        Args:\n",
    "            state, action, reward, state_, done: output of taking one step in the environment\n",
    "        '''\n",
    "        self.memory.store_transition(state, action, reward, state_, done) # store in the memory for replay\n",
    "\n",
    "    def sample_memory(self):\n",
    "        '''\n",
    "            Sample from the memory\n",
    "            Return:\n",
    "                A batch of:\n",
    "                states: Current states\n",
    "                actions: actions under current states:\n",
    "                rewards: rewards for taking action\n",
    "                states_: nest states:\n",
    "                dones : Is next tate a terminal state\n",
    "        '''\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.q_eval.device) # Convert to tensor\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        '''\n",
    "            If learn step counter is multiple of replacing value; \n",
    "            copy state dict of Eval network onto target network\n",
    "        '''\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            #print('Replacing target network')\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                           if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "        self.q_best.save_checkpoint()\n",
    "        \n",
    "    def load_models(self):\n",
    "        print ('Loading Pretrained models...')\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "        self.q_best.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        if self.memory.mem_cntr < self.batch_size: # no nothing if memory is smaller than batch\n",
    "            return 0 # Returning 0 for loss \n",
    "        \n",
    "        self.q_eval.train() # set train mode\n",
    "        self.q_eval.optimizer.zero_grad() # reset optimizer\n",
    "\n",
    "        self.replace_target_network() # call to replace network\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.sample_memory() # get a sample\n",
    "        indices = np.arange(self.batch_size) # Create indices\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions] # get q values for every state under the action\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0] # get max value for next state\n",
    "\n",
    "        q_next[dones] = 0.0 \n",
    "        q_target = rewards + self.gamma*q_next # calculate TD target \n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device) # calculate loss\n",
    "\n",
    "        loss.backward()                                                  # Gradient descent\n",
    "        self.q_eval.optimizer.step() # perform optimizer step\n",
    "        self.learn_step_counter += 1 # increment counter for look for replacement of target network\n",
    "        if self.learn_step_counter % 10 == 0: # reduce epsilon every 10th step\n",
    "            self.decrement_epsilon()\n",
    "        return loss.item()\n",
    "        \n",
    "    def replace_best_network(self):\n",
    "        self.q_best.load_state_dict(self.q_eval.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a3c383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pretrained models...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env, gamma=gamma,\n",
    "                 epsilon=eps,\n",
    "                 lr=lr,\n",
    "                 input_dims=input_dims,\n",
    "                 n_actions=num_actions,\n",
    "                 mem_size=max_mem,\n",
    "                 eps_min=eps_min,\n",
    "                 batch_size=batch_size,\n",
    "                 replace=replace,\n",
    "                 eps_dec=eps_dec,\n",
    "                 model_dir=modelDir,\n",
    "                 algo=algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3140fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_get_filename(rDir, ticker, dType, i):\n",
    "    if i>50:\n",
    "        print ('Too many files... Clean the output dir')\n",
    "        return\n",
    "    fileName = os.path.join(rDir, f'{ticker}_{dType}_action_{i}.png')\n",
    "    if os.path.isfile(fileName):\n",
    "        i += 1\n",
    "        fn_get_filename(rDir, ticker, dType, i)\n",
    "    return fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score:  0.0  average score 0.00 best score -inf epsilon 0.50 steps 32\n",
      "episode:  115 score:  1.0  average score 0.01 best score 0.00 epsilon 0.44 steps 92\n",
      "episode:  161 score:  1.0  average score 0.02 best score 0.01 epsilon 0.42 steps 93\n",
      "episode:  175 score:  1.0  average score 0.03 best score 0.02 epsilon 0.41 steps 39\n",
      "episode:  180 score:  1.0  average score 0.04 best score 0.03 epsilon 0.41 steps 68\n",
      "episode:  213 score:  1.0  average score 0.05 best score 0.04 epsilon 0.39 steps 95\n",
      "episode:  537 score:  1.0  average score 0.06 best score 0.05 epsilon 0.21 steps 99\n",
      "episode:  553 score:  1.0  average score 0.07 best score 0.06 epsilon 0.20 steps 52\n",
      "episode:  567 score:  1.0  average score 0.08 best score 0.07 epsilon 0.19 steps 47\n",
      "episode:  573 score:  1.0  average score 0.09 best score 0.08 epsilon 0.19 steps 90\n",
      "episode:  575 score:  1.0  average score 0.10 best score 0.09 epsilon 0.19 steps 80\n",
      "episode:  577 score:  1.0  average score 0.11 best score 0.10 epsilon 0.19 steps 59\n",
      "episode:  578 score:  1.0  average score 0.12 best score 0.11 epsilon 0.19 steps 85\n",
      "episode:  585 score:  1.0  average score 0.13 best score 0.12 epsilon 0.18 steps 56\n",
      "episode:  586 score:  1.0  average score 0.14 best score 0.13 epsilon 0.18 steps 70\n",
      "episode:  615 score:  1.0  average score 0.15 best score 0.14 epsilon 0.16 steps 53\n",
      "episode:  616 score:  1.0  average score 0.16 best score 0.15 epsilon 0.16 steps 38\n",
      "episode:  619 score:  1.0  average score 0.17 best score 0.16 epsilon 0.16 steps 37\n",
      "episode:  621 score:  1.0  average score 0.18 best score 0.17 epsilon 0.16 steps 56\n",
      "episode:  671 score:  1.0  average score 0.19 best score 0.18 epsilon 0.13 steps 62\n",
      "episode:  672 score:  1.0  average score 0.20 best score 0.19 epsilon 0.13 steps 92\n",
      "episode:  858 score:  1.0  average score 0.21 best score 0.20 epsilon 0.10 steps 99\n",
      "episode:  859 score:  1.0  average score 0.22 best score 0.21 epsilon 0.10 steps 74\n",
      "episode:  862 score:  1.0  average score 0.23 best score 0.22 epsilon 0.10 steps 42\n",
      "episode:  873 score:  1.0  average score 0.24 best score 0.23 epsilon 0.10 steps 29\n",
      "episode:  874 score:  1.0  average score 0.25 best score 0.24 epsilon 0.10 steps 74\n",
      "episode:  885 score:  1.0  average score 0.26 best score 0.25 epsilon 0.10 steps 55\n",
      "episode:  1844 score:  1.0  average score 0.27 best score 0.26 epsilon 0.10 steps 70\n",
      "episode:  2318 score:  1.0  average score 0.28 best score 0.27 epsilon 0.10 steps 63\n",
      "episode:  2371 score:  1.0  average score 0.29 best score 0.28 epsilon 0.10 steps 80\n"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    best_score = -np.inf\n",
    "    history, scores = [],[]\n",
    "    for i in range(EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        n_steps = 0\n",
    "        score = 0\n",
    "        t_loss = 0\n",
    "        hist_dict = {}\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # One hot encode the state        \n",
    "            observation = to_onehot(input_dims, state).reshape(1, input_dims)\n",
    "\n",
    "            # Obtain action\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            # what are the consequences of taking that action?\n",
    "            next_state, reward, terminated, truncated, transmit_prob = env.step(action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            observation_ = to_onehot(input_dims, next_state).reshape(1, input_dims)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action,\n",
    "                                       reward, observation_, done)\n",
    "            loss = agent.learn()\n",
    "            t_loss += loss\n",
    "            state = next_state\n",
    "            n_steps += 1\n",
    "        scores.append(score)\n",
    "        hist_dict['loss'] = t_loss\n",
    "        hist_dict['episode'] = i\n",
    "        hist_dict['score'] = score\n",
    "        hist_dict['epsilon'] = agent.epsilon\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        hist_dict['avg_score'] = avg_score\n",
    "        history.append(hist_dict)\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "                print('episode: ', i,'score: ', score,' average score %.2f' % avg_score, \n",
    "                      'best score %.2f' % best_score, 'epsilon %.2f' % agent.epsilon, \n",
    "                      'steps', n_steps)\n",
    "                agent.replace_best_network()\n",
    "                best_score = avg_score\n",
    "    agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27038957",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    hist_df = pd.DataFrame(history).set_index('episode')\n",
    "    ax = hist_df.plot(y = 'loss')\n",
    "    axt = ax.twinx()\n",
    "    hist_df.plot(y = 'avg_score', ax = axt, c = 'Orange')\n",
    "    axt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab82a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_render_env(env, agent, episodes=10):\n",
    "    rewards = []\n",
    "    state, info = env.reset()\n",
    "    img = plt.imshow(env.render())\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_reward = 0\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            curr_state_encoded = to_onehot(input_dims, state).reshape(1, input_dims)\n",
    "            # propose an action\n",
    "            action = agent.choose_action(curr_state_encoded, evaluation = True)\n",
    "            img.set_data(env.render()) \n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward+=reward\n",
    "            img.set_data(env.render()) \n",
    "            plt.axis('off')\n",
    "            plt.title(f'Episode: {episode}')\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "            display.clear_output(wait=True)\n",
    "            if done:\n",
    "                #print (episode_reward)\n",
    "                break\n",
    "        rewards.append(episode_reward)\n",
    "    env.close()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = fn_render_env(env, agent, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1991f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action mappings - Map actions to numbers\n",
    "action_mappings = {\n",
    "    0: '\\u2190' , # Left\n",
    "    1: '\\u2193', # Down\n",
    "    2: '\\u2192', # Right\n",
    "    3: '\\u2191', # Up\n",
    "}\n",
    "\n",
    "print (action_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value(env):\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    curr_state_encoded = to_onehot(input_dims, state).reshape(1, input_dims)\n",
    "\n",
    "    obs_sqr = int(np.sqrt(input_dims))\n",
    "\n",
    "    initial_state = np.zeros((obs_sqr, obs_sqr))\n",
    "\n",
    "    value_fn = np.zeros((obs_sqr, obs_sqr))\n",
    "\n",
    "    labels = np.empty((obs_sqr, obs_sqr), dtype=object)\n",
    "\n",
    "    for x in range(0, obs_sqr):\n",
    "        for y in range(0, obs_sqr):\n",
    "            my_state = initial_state.copy()\n",
    "\n",
    "            my_state[x,y] = 1  # Place the player at a given X/Y location.\n",
    "\n",
    "            # And now have the critic model predict the state value\n",
    "            # with the player in that location.\n",
    "            pred = agent.q_best(T.tensor(my_state.reshape(1, input_dims), dtype=T.float32, device=agent.q_best.device))\n",
    "            value = T.max(pred).item()\n",
    "            action = T.argmax(pred).item()\n",
    "            value_fn[x,y] = value\n",
    "            labels[x,y] = np.asarray(\"{0}\\n{1:.2f}\".format(action_mappings[action], value))\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    sns.heatmap(value_fn, annot=labels, fmt=\"\", cmap='RdYlGn', ax=axes[0])\n",
    "    axes[1].imshow(env.render())\n",
    "    axes[1].grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61707f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ed693",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration = 1000\n",
    "\n",
    "highscore = 0\n",
    "\n",
    "data_row = []\n",
    "\n",
    "for n_iter in tqdm(range(n_iteration)):\n",
    "\n",
    "    points = 0 # keep track of the reward each episode\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    for i_episode in range(20): # run 20 episodes\n",
    "        \n",
    "        observation = env.reset()[0]\n",
    "        \n",
    "        while True: # run until episode is done\n",
    "            curr_state_encoded = to_onehot(input_dims, observation).reshape(1, input_dims)\n",
    "            \n",
    "            action = agent.choose_action(curr_state_encoded, evaluation = True)\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "            points += reward\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                #env.render()\n",
    "                #print ('Iteration', n_iter, observation, reward, done, info)\n",
    "\n",
    "                if points > highscore: # record high score\n",
    "                    highscore = points\n",
    "                break\n",
    "    data_dict['run'] = n_iter\n",
    "    data_dict['Points']=  points\n",
    "    data_row.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df =  pd.DataFrame(data_row)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.violinplot(x=results_df[\"Points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed4b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
